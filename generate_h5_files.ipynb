{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "broadband-measure",
   "metadata": {},
   "source": [
    "# ML-ready data generation\n",
    "We read pressure data from historical balloon flights, pre-process them, and store them as waveform snippets of the same length in .h5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from obspy.core.utcdatetime import UTCDateTime\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import obspy\n",
    "import os\n",
    "import netCDF4 as nc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-emerald",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-meeting",
   "metadata": {},
   "source": [
    "### Ridgecrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_obspy_trace(times, amp, balloon, starttime, dt, name_maps):\n",
    "                        \n",
    "    #f = interpolate.interp1d(times, amp, kind='linear', )\n",
    "    #times_interp = np.arange(one_tec_data.epoch.values.min()*dt, one_tec_data.epoch.values.max()*dt+dt, dt)\n",
    "    #vTEC = f(times_interp)\n",
    "    tr = obspy.Trace()\n",
    "    tr.data = amp\n",
    "    tr.stats.delta = dt\n",
    "    #tr.stats.network = station\n",
    "    #tr.stats.station = satellite+'ZZZ'+station\n",
    "    tr.stats.station = name_maps[balloon]\n",
    "    tr.stats.starttime = starttime+times[0]\n",
    "    return tr\n",
    "\n",
    "def load_balloon_data(dir_data, starttimes, name_maps):\n",
    "    \n",
    "    datas = {'GPS': obspy.Stream(), 'Baro': obspy.Stream()}\n",
    "    for subdir, dirs, files in os.walk(dir_data):\n",
    "        #print(files)\n",
    "        for file in files:\n",
    "            filepath = subdir + os.sep + file\n",
    "            if not '.csv' in file:\n",
    "                continue\n",
    "                \n",
    "            #if not 'Tortoise' in file:\n",
    "            #    continue\n",
    "            \n",
    "            balloon = file.split('_')[0]\n",
    "            if not balloon in starttimes.keys():\n",
    "                continue\n",
    "            print(balloon)\n",
    "            \n",
    "            data = pd.read_csv(filepath, header=[0])\n",
    "            \n",
    "            type_data = file.split('_')[1].split('.')[0]\n",
    "            \n",
    "            starttime = starttimes[balloon]\n",
    "            times = data['GPSTime(s)'].values\n",
    "            #if balloon == 'Tortoise':\n",
    "            #    print(file)\n",
    "            #    print(times/3600)\n",
    "            dt = times[1]-times[0]\n",
    "            #print(data.columns)\n",
    "            try:\n",
    "                amp = data['WGS84Altitude(m)'].astype(float).values\n",
    "            except:\n",
    "                amp = data[data.columns[-1]].values\n",
    "            tr_data = create_one_obspy_trace(times, amp, balloon, starttime, dt, name_maps)\n",
    "            datas[type_data] += tr_data\n",
    "            \n",
    "    return datas\n",
    "\n",
    "starttimes = {\n",
    "    'Hare': UTCDateTime(2019, 7, 22),\n",
    "    'Tortoise': UTCDateTime(2019, 7, 22),\n",
    "    'Hare2': UTCDateTime(2019, 8, 9),\n",
    "    'CrazyCatLower': UTCDateTime(2019, 8, 9),\n",
    "    'CrazyCatUpper': UTCDateTime(2019, 8, 9),\n",
    "}\n",
    "name_maps = {\n",
    "    'Hare': 'hare',\n",
    "    'Tortoise': 'tort',\n",
    "    'Hare2': 'hare2',\n",
    "    'CrazyCatLower': 'CraLo',\n",
    "    'CrazyCatUpper': 'CraUp',\n",
    "}\n",
    "dir_data = '/projects/infrasound/data/infrasound/2023_ML_balloon/data/Siddharth_balloon/'\n",
    "st_crazycat = load_balloon_data(dir_data, starttimes, name_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/projects/infrasound/data/infrasound/2023_ML_balloon/data/Siddharth_balloon/'\n",
    "st_crazycat['Baro'].write(f\"{folder}st_all.mseed\", format=\"MSEED\")\n",
    "st_crazycat['GPS'].write(f\"{folder}st_all_gps.mseed\", format=\"MSEED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-regulation",
   "metadata": {},
   "source": [
    "### Strateole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "folder = '/projects/infrasound/data/infrasound/2023_ML_balloon/data/Strateole2/'\n",
    "\n",
    "files = []\n",
    "files.append( f'{folder}ST2_C0_01_STR1_TSEN_P1s_v03b.nc' )\n",
    "files.append( f'{folder}ST2_C1_04_STR2_TSEN_P1s_v01.nc' )\n",
    "files.append( f'{folder}ST2_C1_03_TTL4_TSEN_P1s_v01.nc' )\n",
    "files.append( f'{folder}ST2_C1_01_TTL5_TSEN_P1s_v01.nc' )\n",
    "files.append( f'{folder}ST2_C1_16_TTL5_TSEN_P1s_v01.nc' )\n",
    "files.append( f'{folder}ST2_C1_17_TTL3_TSEN_P1s_v01.nc' )\n",
    "\n",
    "st_strateole = obspy.Stream()\n",
    "st_strateole_gps = obspy.Stream()\n",
    "for file in tqdm(files):\n",
    "    dataset = nc.Dataset(file)\n",
    "    file = file.replace('P1s_', '')\n",
    "    dataset_location = nc.Dataset(file)\n",
    "    station = f\"b{file.split('/')[-1].split('_')[2]}{file.split('/')[-1].split('_')[1]}\"\n",
    "\n",
    "    starttime = UTCDateTime(dataset_location.date_start)\n",
    "    time_pressure = dataset.variables['time'][:].filled()\n",
    "    #time_pressure -= time_pressure[0]\n",
    "    pressure = dataset.variables['pressure'][:].filled()\n",
    "    \n",
    "    dt = np.diff(time_pressure)[0]\n",
    "    \n",
    "    tr = obspy.Trace()\n",
    "    tr.data = pressure\n",
    "    tr.stats.station = station\n",
    "    tr.stats.delta = dt\n",
    "    tr.stats.starttime = UTCDateTime(dataset_location.date_start)\n",
    "    \n",
    "    st_strateole += tr\n",
    "    \n",
    "    time_gps = dataset_location.variables['time'][:].filled()\n",
    "    alt = dataset_location.variables['alt'][:].filled()\n",
    "    \n",
    "    dt = np.diff(time_gps)[0]\n",
    "    print(file)\n",
    "    tr = obspy.Trace()\n",
    "    tr.data = alt\n",
    "    tr.stats.station = station\n",
    "    tr.stats.delta = dt\n",
    "    tr.stats.starttime = UTCDateTime(dataset_location.date_start)\n",
    "    \n",
    "    st_strateole_gps += tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-census",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/projects/infrasound/data/infrasound/2023_ML_balloon/data/Strateole2/'\n",
    "st_strateole.write(f\"{folder}st_all.mseed\", format=\"MSEED\")\n",
    "st_strateole_gps.write(f\"{folder}st_all_gps.mseed\", format=\"MSEED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-melissa",
   "metadata": {},
   "outputs": [],
   "source": [
    "strateole=dict(\n",
    "    b01C0 = [UTCDateTime(\"2019-11-13T01:11:48.000000Z\"), UTCDateTime(\"2020-02-27T18:13:28.000000Z\")],\n",
    "    b04C1 = [UTCDateTime(\"2023-09-29T05:34:34.000000Z\"), UTCDateTime(\"2023-10-28T15:30:44.000000Z\")],\n",
    "    b03C1 = [UTCDateTime(\"2023-09-28T23:47:58.000000Z\"), UTCDateTime(\"2023-10-29T22:30:08.000000Z\")],\n",
    "    b01C1 = [UTCDateTime(\"2023-09-27T00:41:36.000000Z\"), UTCDateTime(\"2023-09-28T01:37:16.000000Z\")],\n",
    "    b16C1 = [UTCDateTime(\"2023-12-03T21:53:48.000000Z\"), UTCDateTime(\"2023-12-30T01:36:28.000000Z\")],\n",
    "    b17C1 = [UTCDateTime(\"2023-12-10T01:25:44.000000Z\"), UTCDateTime(\"2023-12-30T07:23:24.000000Z\")]\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "ref_time = st_strateole_gps[0].stats.starttime\n",
    "for tr_gps in st_strateole_gps:\n",
    "    #if \n",
    "    offset = tr_gps.stats.starttime-ref_time\n",
    "    plt.plot(tr_gps.times()+offset, tr_gps.data, label=tr_gps.stats.station)\n",
    "    offset_start, offset_end = tr_gps.stats.starttime-ref_time+1e4, tr_gps.stats.endtime-ref_time-1e4\n",
    "    plt.axvspan(offset_start, offset_end, color='black', alpha=0.3)\n",
    "    template = f'{tr_gps.stats.station} = [UTCDateTime(\"{tr_gps.stats.starttime+offset_start}\"), UTCDateTime(\"{tr_gps.stats.starttime+offset_end}\")]'\n",
    "    print(template)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-swaziland",
   "metadata": {},
   "source": [
    "### Minibooster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "folder = '/projects/infrasound/data/infrasound/2023_ML_balloon/data/MINIBOOSTER_balloon/signal_ascii/'\n",
    "\n",
    "st_minibooster = obspy.Stream()\n",
    "for subdir, dirs, files in os.walk(folder):\n",
    "    for file in tqdm(files):\n",
    "        \n",
    "        filepath = subdir + file\n",
    "        if '.mseed' in file:\n",
    "            continue\n",
    "        \n",
    "        station = file.split('.')[1]\n",
    "        file = open(filepath, 'r')\n",
    "        lines = file.readlines()\n",
    "        \n",
    "        try:\n",
    "            year, doy, hour, minute, second, dt = lines[0].split()\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        tr = obspy.Trace()\n",
    "        tr.data = np.array(lines[1:]).astype(float)\n",
    "        tr.stats.station = station\n",
    "        tr.stats.delta = dt\n",
    "        tr.stats.starttime = UTCDateTime(f'{year}-{doy}T{hour}:{minute}:{second}')\n",
    "        st_minibooster += tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-delivery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "id = 0\n",
    "tr = st_minibooster[id].copy()\n",
    "tr.data = tr.data.filled(0.)\n",
    "tr.detrend()\n",
    "\n",
    "tr_gps = st_minibooster_gps[id].copy()\n",
    "tr_gps.detrend()\n",
    "\n",
    "low_avail = False\n",
    "tr_low = None\n",
    "if low_avail:\n",
    "    tr_low = st_minibooster[id+1].copy()\n",
    "    tr_low.data = tr_low.data.filled(0.)\n",
    "    tr_low.detrend()\n",
    "\n",
    "station = tr.stats.station[:2]\n",
    "\n",
    "#explosion = UTCDateTime('2020-08-20T06:30:00')\n",
    "explosion = tr.stats.starttime + 9260. # explosion 1 b1 b2 b3\n",
    "explosion = tr.stats.starttime + 16180. # explosion 2 b1 b2 b3\n",
    "#explosion = tr.stats.starttime + 24000.-50. # explosion 2 b1\n",
    "offset = explosion-tr.stats.starttime\n",
    "\n",
    "duration = 200.\n",
    "idx_explosion = np.argmin(abs(tr.times()-offset))\n",
    "idx_duration = np.argmin(abs(tr.times()-duration))\n",
    "\n",
    "offset_gps = explosion-tr_gps.stats.starttime\n",
    "idx_explosion_gps = np.argmin(abs(tr_gps.times()-offset_gps))\n",
    "idx_duration_gps = np.argmin(abs(tr_gps.times()-duration))\n",
    "\n",
    "order = 2\n",
    "lowcut = 0.1\n",
    "highcut = 1.\n",
    "sos = scipy.signal.butter(order, [lowcut, highcut], fs=1./tr.stats.delta, btype='band', output='sos')\n",
    "data_filt = scipy.signal.sosfilt(sos, tr.data)\n",
    "sig1 = data_filt[idx_explosion:idx_explosion+idx_duration]\n",
    "if tr_low is not None:\n",
    "    data_filt_low = scipy.signal.sosfilt(sos, tr_low.data)\n",
    "    sig2 = data_filt_low[idx_explosion:idx_explosion+idx_duration]\n",
    "    lags = scipy.signal.correlation_lags(sig1.size, sig2.size, mode='same')\n",
    "    corr = scipy.signal.correlate(sig1/np.linalg.norm(sig1, axis=0), sig2/np.linalg.norm(sig2, axis=0), mode='same')\n",
    "   \n",
    "lowcut = 0.05\n",
    "highcut = 0.49\n",
    "sos = scipy.signal.butter(order, [lowcut, highcut], fs=1./tr_gps.stats.delta, btype='band', output='sos')\n",
    "data_filt_gps = scipy.signal.sosfilt(sos, tr_gps.data)\n",
    "sig1_gps = data_filt_gps[idx_explosion_gps:idx_explosion_gps+idx_duration_gps]\n",
    "#sig1_gps = tr_gps.data#[idx_explosion_gps:idx_explosion_gps+idx_duration_gps]\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "grid = fig.add_gridspec(3, 1)\n",
    "\n",
    "if tr_low is not None:\n",
    "    ax = fig.add_subplot(grid[:2,0])\n",
    "else:\n",
    "    ax = fig.add_subplot(grid[:2,0])\n",
    "ax.plot(tr.times()[idx_explosion:idx_explosion+idx_duration]-offset, sig1, label='high')\n",
    "if tr_low is not None:\n",
    "    ax.plot(tr.times()[idx_explosion:idx_explosion+idx_duration]-offset, sig2, label='low')\n",
    "ax.legend()\n",
    "ax.set_title(f'Balloon {station} - band: {lowcut}-{highcut} Hz')\n",
    "\n",
    "if tr_low is not None:\n",
    "    ax = fig.add_subplot(grid[2,0])\n",
    "    ax.plot(tr.times()[1]*lags, corr)\n",
    "    ax.set_xlabel(f'Time (s) since {explosion}')\n",
    "else:\n",
    "    ax = fig.add_subplot(grid[2,0], sharex=ax)\n",
    "    ax.plot(tr_gps.times()[idx_explosion_gps:idx_explosion_gps+idx_duration_gps]-offset_gps, sig1_gps, '-o')\n",
    "    ax.set_xlabel(f'Time (s) since {explosion}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "minibooster=dict(\n",
    "        B1HI = [UTCDateTime(\"2020-08-20T05:37:51.000000Z\"), UTCDateTime(\"2020-08-20T13:42:51.000000Z\")],\n",
    "        B2HI = [UTCDateTime(\"2020-08-20T05:44:23.000000Z\"), UTCDateTime(\"2020-08-20T13:04:23.000000Z\")],\n",
    "        B2LO = [UTCDateTime(\"2020-08-20T05:44:45.000000Z\"), UTCDateTime(\"2020-08-20T13:54:45.000000Z\")],\n",
    "        B3HI = [UTCDateTime(\"2020-08-20T05:27:50.000000Z\"), UTCDateTime(\"2020-08-20T07:56:10.000000Z\")],\n",
    "        B3LO = [UTCDateTime(\"2020-08-20T05:50:54.000000Z\"), UTCDateTime(\"2020-08-20T10:09:14.000000Z\")]\n",
    "    )\n",
    "\n",
    "plt.figure()\n",
    "ref_time = st_minibooster_gps[0].stats.starttime\n",
    "for tr_gps in st_minibooster_gps:\n",
    "    offset = tr_gps.stats.starttime-ref_time\n",
    "    plt.plot(tr_gps.times()+offset, tr_gps.data, label=tr_gps.stats.station)\n",
    "    #plt.axvline(idx_explosion_gps)\n",
    "    offset_start, offset_end = 0., 0.,\n",
    "    if tr_gps.stats.station == 'B3HI':\n",
    "        offset_start, offset_end = 17100, 26000\n",
    "    if tr_gps.stats.station == 'B3LO':\n",
    "        offset_start, offset_end = 18500, 34000\n",
    "    if tr_gps.stats.station == 'B1HI':\n",
    "        offset_start, offset_end = 18500, 47600\n",
    "    if tr_gps.stats.station == 'B2HI':\n",
    "        offset_start, offset_end = 18500, 44900\n",
    "    if tr_gps.stats.station == 'B2LO':\n",
    "        offset_start, offset_end = 18500, 47900\n",
    "    plt.axvspan(offset_start, offset_end, color='black', alpha=0.3)\n",
    "    template = f'{tr_gps.stats.station} = [UTCDateTime(\"{tr_gps.stats.starttime+offset_start}\"), UTCDateTime(\"{tr_gps.stats.starttime+offset_end}\")]'\n",
    "    print(template)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-method",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/projects/infrasound/data/infrasound/2023_ML_balloon/data/MINIBOOSTER_balloon/signal_ascii/'\n",
    "st_minibooster.write(f\"{folder}st_all.mseed\", format=\"MSEED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-treasurer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from obspy.core.utcdatetime import UTCDateTime\n",
    "\n",
    "folder = '/projects/infrasound/data/infrasound/2023_ML_balloon/data/MINIBOOSTER_balloon/balloon_trajectories/'\n",
    "\n",
    "st_minibooster_gps = obspy.Stream()\n",
    "for subdir, dirs, files in os.walk(folder):\n",
    "    for file in tqdm(files):\n",
    "        filepath = subdir + file\n",
    "        \n",
    "        if '.mseed' in file:\n",
    "            continue\n",
    "        \n",
    "        station = file.split('_')[0]\n",
    "        #print(station)\n",
    "        df = pd.read_csv(filepath, delimiter='\\n', header=None, names=['text'])\n",
    "\n",
    "        # Define a regular expression pattern to extract the required fields\n",
    "        pattern = r'\\s+gps-time=(\\S+)\\s+gps-leap=\\S+\\s+iers-leap=\\S+\\s+lat=([\\+\\-]?\\d+\\.\\d+)\\s+lon=([\\+\\-]?\\d+\\.\\d+)\\s+elev=([\\d\\.]+)'\n",
    "        \n",
    "        # Use the str.extract method with the pattern to extract the fields into new columns\n",
    "        df[['gps-time', 'lat', 'lon', 'elev']] = df['text'].str.extract(pattern)\n",
    "        df.loc[:,'gps-time'] = pd.to_datetime(df.loc[:,'gps-time'])\n",
    "        df.loc[:,'lat'] = df.loc[:,'lat'].str[1:].astype(float)\n",
    "        df.loc[:,'lon'] = df.loc[:,'lon'].str[1:].astype(float)\n",
    "        df.loc[:,'elev'] = df.loc[:,'elev'].astype(float)\n",
    "        df = df.drop(columns=['text'])\n",
    "        \n",
    "        dt = (df.iloc[1]['gps-time'] - df.iloc[0]['gps-time']).total_seconds()\n",
    "        starttime = UTCDateTime(df['gps-time'].iloc[0])\n",
    "        \n",
    "        tr = obspy.Trace()\n",
    "        tr.data = df.elev.values\n",
    "        #print(tr.data)\n",
    "        tr.stats.station = station\n",
    "        tr.stats.delta = dt\n",
    "        tr.stats.starttime = starttime\n",
    "        \n",
    "        st_minibooster_gps += tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/projects/infrasound/data/infrasound/2023_ML_balloon/data/MINIBOOSTER_balloon/balloon_trajectories/'\n",
    "st_minibooster_gps.write(f\"{folder}st_all_gps.mseed\", format=\"MSEED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-foster",
   "metadata": {},
   "source": [
    "### Starliner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/projects/infrasound/data/infrasound/2023_ML_balloon/data/starliner_scrub/20210803_starliner_scrub/balloon_data/'\n",
    "\n",
    "st_starliner = obspy.Stream()\n",
    "datas = {}\n",
    "for subdir, dirs, files in os.walk(folder):\n",
    "    for file in tqdm(files):\n",
    "        filepath = subdir + os.sep + file\n",
    "        \n",
    "        if 'DDF' in file:\n",
    "            continue\n",
    "          \n",
    "        if '.mseed' in file:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            tr = obspy.read(filepath)[0]\n",
    "            balloon = tr.stats.station[4:]\n",
    "            channel = tr.stats.channel[-1]\n",
    "            ext = 'HI'\n",
    "            if channel == 'B' or channel == '2':\n",
    "                ext = 'LO'\n",
    "            station = f'b{balloon}{ext}'\n",
    "            tr.stats.station = station\n",
    "            st_starliner += tr\n",
    "        except:\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/projects/infrasound/data/infrasound/2023_ML_balloon/data/starliner_scrub/20210803_starliner_scrub/balloon_data/'\n",
    "st_starliner.write(f\"{folder}st_all.mseed\", format=\"MSEED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/projects/infrasound/data/infrasound/2023_ML_balloon/data/starliner_scrub/20210803_starliner_scrub/trajectory_data/'\n",
    "\n",
    "st_starliner_gps = obspy.Stream()\n",
    "datas = {}\n",
    "for subdir, dirs, files in os.walk(folder):\n",
    "    for file in tqdm(files):\n",
    "        file_name = file\n",
    "        filepath = subdir + os.sep + file#BNG01\n",
    "        balloon = subdir.split('/')[-2].split('_')[1]\n",
    "    \n",
    "        \n",
    "        if '.mseed' in file:\n",
    "            continue\n",
    "        \n",
    "        channel = file.split('.')[0].split('_')\n",
    "        \n",
    "        ext = 'HI'\n",
    "        if len(channel) > 1:\n",
    "            if channel[1] == 'a' or channel[1] == '2':\n",
    "                ext = 'LO'\n",
    "        balloon = f'b{balloon}{ext}'\n",
    "        \n",
    "        print(channel)\n",
    "        if len(channel) > 1:\n",
    "            channel = channel[0][:4]+channel[1][:1]\n",
    "        else:\n",
    "            channel = channel[0][:5]\n",
    "            \n",
    "        print(filepath)\n",
    "        if 'bounder' in file:\n",
    "            #    pd.read_csv(filepath, skiprows=34, header=[0])\n",
    "            #else:\n",
    "            data = pd.read_csv(filepath, skiprows=67, header=None, names=['AA', 'SEF', 'HTR', 'BW0', 'BW1', 'MET','MAP','PAR','GAR','GF1','GF2','EXT','Pres (hPa)','dP (hPa/line)','Temp (C)','Batt (V)','Cap (V)','GPS Flag (HEX)','GPS SV','Lon','Lat','Alt (m)','Date','Time','gSpeed (m/s)','aRate (m/s)','aFilt (m/s)','heading (deg)', 'N/A'])\n",
    "            data = data.loc[:,['Lon','Lat','Alt (m)','Date','Time']]\n",
    "            year = data['Date'].astype(str).str[:4]\n",
    "            month = data['Date'].astype(str).str[4:6]\n",
    "            day = data['Date'].astype(str).str[6:]\n",
    "            data.loc[:,'Date'] = pd.to_datetime(year+'-'+month+'-'+day+'T'+data['Time'])\n",
    "            \n",
    "        elif 'gps' in file:\n",
    "            df = pd.read_csv(filepath, delimiter='\\n', header=None, names=['text'])\n",
    "\n",
    "            # Define a regular expression pattern to extract the required fields\n",
    "            pattern = r'utc-time=(\\S+) lat=([\\+\\-]?\\d+\\.\\d+) lon=([\\+\\-]?\\d+\\.\\d+) elev=([\\d\\.]+)'\n",
    "            pattern = r'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}|[+-]?\\d+\\.\\d+'\n",
    "            pattern = r'utc-time=(\\S+)\\s+gps-time=\\S+\\s+gps-leap=\\S+\\s+iers-leap=\\S+\\s+lat=([\\+\\-]?\\d+\\.\\d+)\\s+lon=([\\+\\-]?\\d+\\.\\d+)\\s+elev=([\\d\\.]+)'\n",
    "            #print(df['text'].str.extract(pattern))\n",
    "            # Use the str.extract method with the pattern to extract the fields into new columns\n",
    "            df[['gps-time', 'lat', 'lon', 'elev']] = df['text'].str.extract(pattern)\n",
    "\n",
    "            # Convert the columns to appropriate data types\n",
    "            df['lat'] = df['lat'].astype(float)\n",
    "            df['lon'] = df['lon'].astype(float)\n",
    "            df['elev'] = df['elev'].astype(float)\n",
    "            df['gps-time'] = pd.to_datetime(df['gps-time'])\n",
    "\n",
    "            # Drop the original text column\n",
    "            data = df.drop(columns=['text'])\n",
    "            data.columns = ['Date', 'Lat', 'Lon', 'Alt (m)',]\n",
    "        else:\n",
    "            file = open(filepath, 'r')\n",
    "            lines = file.readlines()\n",
    "            i = 0\n",
    "            not_found = True\n",
    "            while not_found:\n",
    "                if 'Time' in lines[i]:\n",
    "                    not_found = False\n",
    "                i+=1\n",
    "                \n",
    "            locs = []\n",
    "            for j in range (0, len(lines)):\n",
    "                if 'Longitude' in lines[j]:\n",
    "                    locs.append(j)\n",
    "            locs += [len(lines)+10]\n",
    "            #data = pd.read_csv(filepath, skiprows=i-1, header=[0])\n",
    "            #data.dropna(subset=['Longitude '], inplace=True)\n",
    "            #print(data)\n",
    "            \n",
    "            #cols = ['Date    ','Time    ','Latitude  ','Longitude ','Head','Km/h','Alt-m  ','Lock','N/A1','Temp C','Pa    ','N/A0','Temp C','Pa    ','N/A']\n",
    "            for isep in range(1,len(locs)):\n",
    "                \n",
    "                data = pd.DataFrame(lines[locs[isep-1]:locs[isep]-10])\n",
    "                data = data[0].str.split(',', expand=True)\n",
    "                cols = data.iloc[0].values\n",
    "                data.columns = cols\n",
    "                data = data.iloc[1:]\n",
    "                #print(data)\n",
    "                try:\n",
    "                    #data['Date    '] = data['Date    '].str.replace('/', '-')\n",
    "                    data['Date    '] = '2021-08-'+data['Date    '].str.split('/').str[1]\n",
    "                    data['Date    '] = pd.to_datetime(data['Date    ']+'T'+data['Time    '])\n",
    "                    data = data.loc[:,['Date    ', 'Latitude  ', 'Longitude ', 'Alt-m  ']]\n",
    "                    data.columns = ['Date', 'Lat', 'Lon', 'Alt (m)',]\n",
    "                    data.loc[:,'Alt (m)'] = data.loc[:,'Alt (m)'].str[1:].astype(float)\n",
    "                except:\n",
    "                    #data['UTC Date'] = data['UTC Date'].str.replace('/', '-')\n",
    "                    data['UTC Date'] = '2021-08-'+data['UTC Date'].str.split('/').str[1]\n",
    "                    data['UTC Date'] = pd.to_datetime(data['UTC Date']+'T'+data['UTC Time'])\n",
    "                    data = data.loc[:,['UTC Date', 'Latitude  ', 'Longitude ', 'Alt-m  ']]\n",
    "                    data.columns = ['Date', 'Lat', 'Lon', 'Alt (m)',]\n",
    "                    #print(data.loc[:,'Lat'].str[2:])\n",
    "                    data.loc[:,'Lat'] = data.loc[:,'Lat'].str[2:].astype(float)\n",
    "                    data.loc[:,'Alt (m)'] = data.loc[:,'Alt (m)'].str[1:].astype(float)\n",
    "\n",
    "                #data['Alt (m)'] = data['Alt (m)'].str[1:].astype(float)\n",
    "            \n",
    "        dt = (data.iloc[1]['Date'] - data.iloc[0]['Date']).total_seconds()\n",
    "        starttime = UTCDateTime(data['Date'].iloc[0])\n",
    "            \n",
    "        tr = obspy.Trace()\n",
    "        tr.data = data['Alt (m)'].values\n",
    "        tr.stats.station = balloon\n",
    "        tr.stats.channel = channel\n",
    "        tr.stats.delta = dt\n",
    "        tr.stats.starttime = starttime\n",
    "            \n",
    "        st_starliner_gps += tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-authority",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/projects/infrasound/data/infrasound/2023_ML_balloon/data/starliner_scrub/20210803_starliner_scrub/trajectory_data/'\n",
    "st_starliner_gps.write(f\"{folder}st_all_gps.mseed\", format=\"MSEED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-backup",
   "metadata": {},
   "outputs": [],
   "source": [
    "starliner=dict(\n",
    "    b1LO = [UTCDateTime(\"2021-08-04T18:48:14.000000Z\"), UTCDateTime(\"2021-08-05T00:46:34.000000Z\")],\n",
    "    b1HI = [UTCDateTime(\"2021-08-04T18:48:08.000000Z\"), UTCDateTime(\"2021-08-05T03:54:48.000000Z\")],\n",
    "    b2HI = [UTCDateTime(\"2021-08-03T16:19:04.000000Z\"), UTCDateTime(\"2021-08-04T02:06:04.000000Z\")],\n",
    "    b2LO = [UTCDateTime(\"2021-08-03T16:23:52.000000Z\"), UTCDateTime(\"2021-08-04T02:10:52.000000Z\")],\n",
    "    b3HI = [UTCDateTime(\"2021-08-03T16:23:13.000000Z\"), UTCDateTime(\"2021-08-04T01:58:13.000000Z\")],\n",
    "    b4HI = [UTCDateTime(\"2021-08-03T17:23:13.000000Z\"), UTCDateTime(\"2021-08-04T01:21:43.000000Z\")]\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "ref_time = st_starliner_gps[-1].stats.starttime\n",
    "offset_start, offset_end = 0., 0.\n",
    "for tr_gps in st_starliner_gps:\n",
    "    if tr_gps.stats.station == 'b3HI':\n",
    "        offset_start, offset_end = 18370, 52870\n",
    "    if tr_gps.stats.station == 'b2LO' or tr_gps.stats.station == 'b2HI':\n",
    "        offset_start, offset_end = 17650, 52870\n",
    "    if tr_gps.stats.station == 'b4HI':\n",
    "        offset_start, offset_end = 21990, 50700\n",
    "    if tr_gps.stats.station == 'b1HI':\n",
    "        offset_start, offset_end = 56900., 89700\n",
    "    if tr_gps.stats.station == 'b1LO':\n",
    "        offset_start, offset_end = 56900., 78400\n",
    "    offset = tr_gps.stats.starttime-ref_time\n",
    "    plt.plot(tr_gps.times()+offset, tr_gps.data, label=tr_gps.stats.station)\n",
    "    plt.axvspan(offset_start, offset_end, color='black', alpha=0.3)\n",
    "    template = f'{tr_gps.stats.station} = [UTCDateTime(\"{tr_gps.stats.starttime+offset_start}\"), UTCDateTime(\"{tr_gps.stats.starttime+offset_end}\")]'\n",
    "    print(template)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-november",
   "metadata": {},
   "source": [
    "### Building validation/training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-voluntary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_st(st_all, duration, return_times, overlap=1., freq_min=0.01, freq_max=20.,):\n",
    "    \n",
    "    new_st = obspy.Stream()\n",
    "    id = -1\n",
    "    sections = dict()\n",
    "    sections_ids = dict()\n",
    "    sections_unique_ids = dict()\n",
    "    cpt_section_ids = -1\n",
    "    print('Trimming streams')\n",
    "    for itr, tr_loop in tqdm(enumerate(st_all), total=len(st_all), disable=True):\n",
    "        \n",
    "        if itr > 5:\n",
    "            continue\n",
    "        \n",
    "        starttime, endtime = return_times[itr]\n",
    "        tr_in = tr_loop.copy()\n",
    "        tr_in.trim(starttime=starttime, endtime=endtime)\n",
    "        \n",
    "        if isinstance(tr_in.data, np.ma.MaskedArray):\n",
    "            unmasked_idx = np.where(~tr_in.data.mask)[0]\n",
    "            diff_unmasked_idx = np.where(np.diff(unmasked_idx)>1)[0]\n",
    "            last_idx = unmasked_idx[0]\n",
    "        else:\n",
    "            unmasked_idx = np.arange(tr_in.data.size)\n",
    "            diff_unmasked_idx = np.array([], dtype=int)\n",
    "            last_idx = 0\n",
    "        diff_unmasked_idx = np.r_[diff_unmasked_idx, -1]\n",
    "        \n",
    "        sections[tr_in.stats.station] = []\n",
    "        sections_ids[tr_in.stats.station] = []\n",
    "        sections_unique_ids[tr_in.stats.station] = []\n",
    "        skipped_windows = 0\n",
    "        total_windows = 0\n",
    "        for id_section, idx in enumerate(diff_unmasked_idx):\n",
    "            \n",
    "            cpt_section_ids += 1\n",
    "            sections_unique_ids[tr_in.stats.station].append(cpt_section_ids)\n",
    "            tr = tr_in.copy()\n",
    "            current_idx = unmasked_idx[idx]\n",
    "            tr.data = tr.data[last_idx:current_idx]\n",
    "            #print(last_idx, current_idx, unmasked_idx[diff_unmasked_idx[0]+1])\n",
    "            last_idx = unmasked_idx[idx+1]\n",
    "            #print(np.ma.is_masked(tr.data))\n",
    "            \n",
    "            sections[tr_in.stats.station].append( (tr.stats.starttime, tr.stats.endtime) )\n",
    "            \n",
    "            tr.stats.starttime\n",
    "            tr.detrend()\n",
    "            tr.filter('bandpass', freqmin=freq_min, freqmax=freq_max, zerophase=True, corners=6)\n",
    "            #tr.resample(freq_max*2)\n",
    "            starttimes = np.arange(0., tr.times()[-1], duration*overlap)\n",
    "            \n",
    "            sections_ids[tr_in.stats.station].append([])\n",
    "            for starttime in tqdm(starttimes):\n",
    "                id += 1\n",
    "                tr_loc_Baro = tr.copy()\n",
    "                tr_loc_Baro.resample(freq_max*2)\n",
    "                tr_loc_Baro.trim(starttime=tr.stats.starttime+starttime, endtime=tr.stats.starttime+starttime+duration)\n",
    "                tr_loc_Baro.stats.station = tr.stats.station + '-' + str(id)\n",
    "                total_windows += 1\n",
    "                if tr_loc_Baro.stats.endtime-tr_loc_Baro.stats.starttime<duration:\n",
    "                    skipped_windows += 1\n",
    "                    continue\n",
    "                new_st += tr_loc_Baro\n",
    "                sections_ids[tr_in.stats.station][id_section].append(tr_loc_Baro.stats.station)\n",
    "            \n",
    "        if skipped_windows > 0:\n",
    "            print(f'{tr_loop.stats.station}: Skipping {skipped_windows}/{total_windows} windows')\n",
    "            \n",
    "    return new_st, sections, sections_ids, sections_unique_ids\n",
    "    \n",
    "from math import log, ceil, floor\n",
    "def closest_power(x, power=8):\n",
    "    possible_results = floor(log(x, power)), ceil(log(x, power))\n",
    "    return min(possible_results, key= lambda z: abs(x-power**z))\n",
    "   \n",
    "def find_closest_duration(target_duration, target_sampling):\n",
    "    nsize = int(target_sampling*target_duration)\n",
    "    power = closest_power(nsize, power=2)\n",
    "    nsize = 2**power\n",
    "    duration = nsize/target_sampling\n",
    "    return duration\n",
    "    \n",
    "def get_times(st, flight_id):\n",
    "    \n",
    "    #l_starttimes, l_endtimes = [tr.stats.starttime for tr in st], [tr.stats.endtime for tr in st.copy()]\n",
    "    #starttime, endtime = min(l_starttimes), max(l_endtimes)\n",
    "    #default_times = (starttime, endtime)\n",
    "    \n",
    "    available_times = dict(\n",
    "        ridgecrest=dict(\n",
    "            CraLo = [UTCDateTime('2019-08-09T18:52:00'), UTCDateTime('2019-08-10T02:43:31')],\n",
    "            CraUp = [UTCDateTime('2019-08-09T18:52:00'), UTCDateTime('2019-08-10T02:43:31')],\n",
    "            tort = [UTCDateTime('2019-07-22T14:09:00'), 'default']\n",
    "        ),\n",
    "        minibooster=dict(\n",
    "            B1HI = [UTCDateTime(\"2020-08-20T05:37:51.000000Z\"), UTCDateTime(\"2020-08-20T13:42:51.000000Z\")],\n",
    "            B2HI = [UTCDateTime(\"2020-08-20T05:44:23.000000Z\"), UTCDateTime(\"2020-08-20T13:04:23.000000Z\")],\n",
    "            B2LO = [UTCDateTime(\"2020-08-20T05:44:45.000000Z\"), UTCDateTime(\"2020-08-20T13:54:45.000000Z\")],\n",
    "            B3HI = [UTCDateTime(\"2020-08-20T05:27:50.000000Z\"), UTCDateTime(\"2020-08-20T07:56:10.000000Z\")],\n",
    "            B3LO = [UTCDateTime(\"2020-08-20T05:50:54.000000Z\"), UTCDateTime(\"2020-08-20T10:09:14.000000Z\")]\n",
    "        ),\n",
    "        strateole=dict(\n",
    "            b01C0 = [UTCDateTime(\"2019-11-13T01:11:48.000000Z\"), UTCDateTime(\"2020-02-27T18:13:28.000000Z\")],\n",
    "            b04C1 = [UTCDateTime(\"2023-09-29T05:34:34.000000Z\"), UTCDateTime(\"2023-10-28T15:30:44.000000Z\")],\n",
    "            b03C1 = [UTCDateTime(\"2023-09-28T23:47:58.000000Z\"), UTCDateTime(\"2023-10-29T22:30:08.000000Z\")],\n",
    "            b01C1 = [UTCDateTime(\"2023-09-27T00:41:36.000000Z\"), UTCDateTime(\"2023-09-28T01:37:16.000000Z\")],\n",
    "            b16C1 = [UTCDateTime(\"2023-12-03T21:53:48.000000Z\"), UTCDateTime(\"2023-12-30T01:36:28.000000Z\")],\n",
    "            b17C1 = [UTCDateTime(\"2023-12-10T01:25:44.000000Z\"), UTCDateTime(\"2023-12-30T07:23:24.000000Z\")]\n",
    "        ),\n",
    "        starliner=dict(\n",
    "            b1LO = [UTCDateTime(\"2021-08-04T18:48:14.000000Z\"), UTCDateTime(\"2021-08-05T00:46:34.000000Z\")],\n",
    "            b1HI = [UTCDateTime(\"2021-08-04T18:48:08.000000Z\"), UTCDateTime(\"2021-08-05T03:54:48.000000Z\")],\n",
    "            b2HI = [UTCDateTime(\"2021-08-03T16:19:04.000000Z\"), UTCDateTime(\"2021-08-04T02:06:04.000000Z\")],\n",
    "            b2LO = [UTCDateTime(\"2021-08-03T16:23:52.000000Z\"), UTCDateTime(\"2021-08-04T02:10:52.000000Z\")],\n",
    "            b3HI = [UTCDateTime(\"2021-08-03T16:23:13.000000Z\"), UTCDateTime(\"2021-08-04T01:58:13.000000Z\")],\n",
    "            b4HI = [UTCDateTime(\"2021-08-03T17:23:13.000000Z\"), UTCDateTime(\"2021-08-04T01:21:43.000000Z\")]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return_times = []\n",
    "    for tr in st:\n",
    "        default_time = (tr.stats.starttime, tr.stats.endtime)\n",
    "        return_times.append( default_time )\n",
    "    if flight_id in available_times:\n",
    "        for itr, tr in enumerate(st):\n",
    "            if tr.stats.station in available_times[flight_id]:\n",
    "                default_time = [tr.stats.starttime, tr.stats.endtime]\n",
    "                return_times[itr] = available_times[flight_id][tr.stats.station]\n",
    "                if return_times[itr][0] == 'default':\n",
    "                    return_times[itr][0] = default_time[0]\n",
    "                if return_times[itr][1] == 'default':\n",
    "                    return_times[itr][1] = default_time[1]\n",
    "    \n",
    "    return return_times\n",
    "    \n",
    "target_duration = 150.\n",
    "overlap = 0.25\n",
    "freq_min = 0.15\n",
    "freq_max = 2.5\n",
    "target_sampling = freq_max*2.\n",
    "\"\"\"\n",
    "st = st_crazycat['Baro'].copy()\n",
    "flight_id = 'ridgecrest'\n",
    "event_type = 'earthquake'\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "st = st_strateole.copy()\n",
    "flight_id = 'strateole'\n",
    "event_type = 'earthquake'\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "st = st_minibooster.copy()\n",
    "st.merge()\n",
    "flight_id = 'minibooster'\n",
    "event_type = 'explosion'\n",
    "\"\"\"\n",
    "\n",
    "st = st_starliner.copy()\n",
    "st.merge()\n",
    "flight_id = 'starliner'\n",
    "event_type = 'explosion'\n",
    "\n",
    "\n",
    "return_times = get_times(st, flight_id)\n",
    "duration = find_closest_duration(target_duration, target_sampling)\n",
    "target_size = int(duration*target_sampling)\n",
    "new_st, sections, sections_ids, sections_unique_ids = trim_st(st, duration, return_times, overlap=overlap, freq_min=freq_min, freq_max=freq_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def store_all_as_hdf5(datasets, st_all, target_size, flight_id, datasets_section_ids, event_type='earthquake', new_st_label=None):\n",
    "\n",
    "    \"\"\"\n",
    "    ## Initialization dataset\n",
    "    size_crop = st_all['Baro'][0].data.size-int(st_all['Baro'][0].data.size*crop_percent)\n",
    "    there_are_GPS_data = True if len(st_all['GPS']) > 0 else False\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a subgroup for each stream with the corresponding event ID as the subgroup name\n",
    "    for dataset in datasets:\n",
    "    \n",
    "        results = {'X': [], 'label': [], 'event_type': [], 'window': [], 'id': [], 'station': [], 'flight_id': [], 'section_id': []}\n",
    "        print(f'Building dataset {dataset}')\n",
    "        \n",
    "        \"\"\"\n",
    "        idmin, idmax = datasets[dataset]\n",
    "        idmin, idmax = int(idmin), int(idmax)\n",
    "        st_Baro_loc = st_all['Baro'][idmin:idmax]\n",
    "        if there_are_GPS_data:\n",
    "            st_GPS_loc = st_all['GPS'][idmin:idmax]\n",
    "        else:\n",
    "            st_GPS_loc = st_Baro_loc\n",
    "        for tr_Baro_loc_in, tr_GPS_loc in zip(st_Baro_loc, st_GPS_loc):\n",
    "        \"\"\"\n",
    "        for istation, station in tqdm(enumerate(datasets[dataset]), total=len(datasets[dataset])):\n",
    "            \n",
    "            tr_label = None\n",
    "            if new_st_label is not None:\n",
    "                tr_label = new_st_label.select(station=station)[0].copy()\n",
    "            \n",
    "            section_id = datasets_section_ids[dataset][istation]\n",
    "            \n",
    "            tr = new_st.select(station=station)[0].copy()\n",
    "            window = (str(tr.stats.starttime), str(tr.stats.endtime))\n",
    "            \n",
    "            \"\"\"\n",
    "            tr_Baro_loc_cropped = tr_Baro_loc.copy()\n",
    "            if size_crop > 0:\n",
    "                tr_Baro_loc_cropped.data[size_crop//2:-size_crop//2] = 0.\n",
    "            \"\"\"\n",
    "            if abs(tr.data).max() == 0.:\n",
    "                print('Problem amplitude')\n",
    "                #print(tr_Baro_loc.stats.station)\n",
    "                #print(X0.shape, X1.shape)\n",
    "                continue\n",
    "            \n",
    "            X = np.expand_dims(tr.data, axis=-1)\n",
    "            \"\"\"\n",
    "            X0 = np.expand_dims(tr_Baro_loc_cropped.data, axis=-1)\n",
    "            if there_are_GPS_data:\n",
    "                X1 = np.expand_dims(tr_GPS_loc.data, axis=-1)\n",
    "                if not X0.shape[0] == X1.shape[0]:\n",
    "                    #print(tr_Baro_loc.stats.station)\n",
    "                    #print(X0.shape, X1.shape)\n",
    "                    continue\n",
    "                X = np.concatenate((X0, X1), axis=-1)\n",
    "            else:\n",
    "                X = X0\n",
    "            \"\"\"\n",
    "            X = X[:target_size,:]\n",
    "            \n",
    "            if tr_label is None:\n",
    "                label = np.zeros_like(X)\n",
    "            else:\n",
    "                label = np.expand_dims(tr_label.data, axis=-1)\n",
    "            label = label[:target_size,:]\n",
    "            \n",
    "            if X.shape[0] < target_size or label.shape[0] < target_size:\n",
    "                print('problem size')\n",
    "                print(tr.stats.station)\n",
    "                print(X.shape)\n",
    "                continue\n",
    "            \n",
    "            if np.isnan(X).any():\n",
    "                print('problem nan')\n",
    "                print(tr.stats.station)\n",
    "                continue\n",
    "            \n",
    "            results['X'].append( X )\n",
    "            results['label'].append( label )\n",
    "            results['event_type'].append( event_type )\n",
    "            results['window'].append( window )\n",
    "            results['flight_id'].append( flight_id )\n",
    "            results['station'].append( tr.stats.station.split('-')[0] )\n",
    "            results['id'].append( tr.stats.station )\n",
    "            results['section_id'].append( section_id )\n",
    "        \n",
    "        # Open the HDF5 file in \"write\" mode\n",
    "        with h5py.File(filename.format(dataset=dataset, flight_id=flight_id), \"w\") as f:\n",
    "            f.create_dataset('X', data=results['X'], dtype='float32')\n",
    "            f.create_dataset('label', data=results['label'], dtype='float32')\n",
    "            f.create_dataset('event_type', data=str(results['event_type']))\n",
    "            f.create_dataset('window', data=results['window'])\n",
    "            f.create_dataset('flight_id', data=results['flight_id'])\n",
    "            f.create_dataset('station', data=results['station'])\n",
    "            f.create_dataset('id', data=results['id'])\n",
    "            f.create_dataset('section_id', data=results['section_id'])\n",
    "            \n",
    "    return results\n",
    "\n",
    "def find_bounds_dataset_section(list_quantiles, requested_dataset, l_stations):\n",
    "    \n",
    "    iprev = 0\n",
    "    for dataset, quantile in list_quantiles.items():\n",
    "        icurrent = int(len(l_stations)*quantile)\n",
    "        idx = np.arange(len(l_stations))[iprev:iprev+icurrent]\n",
    "        iprev = icurrent\n",
    "        if dataset == requested_dataset:\n",
    "            return idx.min(), idx.max()\n",
    "\n",
    "def prepare_datasets_dates(new_st, sections, sections_ids, sections_unique_ids, list_quantiles, target_size, min_size=10):\n",
    "    \n",
    "    datasets = dict()\n",
    "    datasets_section_ids = dict()\n",
    "    for dataset, quantile in tqdm(list_quantiles.items()):\n",
    "        \n",
    "        datasets[dataset] = []\n",
    "        datasets_section_ids[dataset] = []\n",
    "        for station in sections_ids: # Loop over each flight\n",
    "            l_sections = sections_ids[station]\n",
    "            for i_section, l_stations in enumerate(l_sections): # Loop over each segment in each flight\n",
    "                section_unique_id = sections_unique_ids[station][i_section]\n",
    "                i_min, i_max = find_bounds_dataset_section(list_quantiles, dataset, l_stations)\n",
    "                starttime, endtime = sections[station][i_section]\n",
    "                datasets[dataset] += l_stations[i_min:i_max]\n",
    "                datasets_section_ids[dataset] += [section_unique_id for _ in l_stations[i_min:i_max]]\n",
    "                \n",
    "    for dataset in list_quantiles:\n",
    "        print(f'dataset \"{dataset}\" ({len(datasets[dataset])} inputs):')\n",
    "    \n",
    "    return datasets, datasets_section_ids\n",
    "\n",
    "new_st_label = None\n",
    "filename = '/projects/infrasound/data/infrasound/2023_ML_balloon/data/{dataset}_waveform_dataset_{flight_id}_0.015Hz.h5'\n",
    "list_quantiles = dict(all=1.)\n",
    "\n",
    "datasets, datasets_section_ids = prepare_datasets_dates(new_st, sections, sections_ids, sections_unique_ids, list_quantiles, target_size)\n",
    "results = store_all_as_hdf5(datasets, new_st, target_size, flight_id, datasets_section_ids, event_type=event_type, new_st_label=new_st_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-verification",
   "metadata": {},
   "source": [
    "## Test tsfresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def split_time_series(df, duration, overlap, sample_rate):\n",
    "    # Convert 'time' to datetime if not already converted and ensure consistent index\n",
    "    #df['time'] = pd.to_datetime(df['time'])\n",
    "    #df.set_index('time', inplace=True)\n",
    "    df = df.sort_index()\n",
    "\n",
    "    # Determine the sampling interval from the first two timestamps\n",
    "    #sample_rate = (df.index[1] - df.index[0]).total_seconds()\n",
    "    samples_per_chunk = int(duration / sample_rate)\n",
    "    \n",
    "    overlap_samples = int(samples_per_chunk * overlap )\n",
    "    step_size = samples_per_chunk - overlap_samples\n",
    "\n",
    "    # Create an array of start indices for each sub-series\n",
    "    start_indices = np.arange(0, len(df) - samples_per_chunk + 1, step_size)\n",
    "    \n",
    "    # Generate sub-series by slicing df based on start_indices\n",
    "    def slice_chunk(start_idx, chunk_id):\n",
    "        chunk = df.iloc[start_idx:start_idx + samples_per_chunk].copy()\n",
    "        chunk['id'] = chunk_id\n",
    "        return chunk\n",
    "    \n",
    "    # Use list comprehension to create sub-series efficiently\n",
    "    sub_series = []\n",
    "    for i, idx in tqdm(enumerate(start_indices), total=len(start_indices)):\n",
    "        sub_series.append( slice_chunk(idx, i + 1) )\n",
    "\n",
    "    # Combine all sub-series into a single DataFrame\n",
    "    return pd.concat(sub_series)\n",
    "\n",
    "## Test tsfresh\n",
    "start_constant_alt = UTCDateTime('2019-07-22T14:09:00') # Tortoise\n",
    "end_constant_alt = st_all['Baro'][0].stats.endtime # Tortoise\n",
    "tr = st_all['Baro'][0].copy()\n",
    "tr.resample(10.)\n",
    "tr.filter('highpass', freq=0.25)\n",
    "tr.trim(starttime=start_constant_alt, endtime=end_constant_alt)\n",
    "\n",
    "pd_Baro = pd.DataFrame(np.c_[tr.times(), tr.data], columns=['time', 'pressure'])\n",
    "duration = 50.\n",
    "overlap = 0.25\n",
    "sample_rate = tr.stats.delta\n",
    "pd_Baro_total = split_time_series(pd_Baro, duration, overlap, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-velvet",
   "metadata": {},
   "source": [
    "## Celine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_constant_alt = UTCDateTime('2019-08-09T18:52:00')\n",
    "end_constant_alt = UTCDateTime('2019-08-10T02:43:31')\n",
    "tr_upper = st_all['GPS'].copy()\n",
    "for tr in tr_upper:\n",
    "    tr.data = tr.data.astype(float)\n",
    "tr_upper[0].stats.station = 'low'\n",
    "tr_upper[1].stats.station = 'up'\n",
    "\n",
    "plt.figure()\n",
    "for tr in tr_upper:\n",
    "    plt.plot(tr.times(), tr.data, label=tr.stats.station)\n",
    "plt.legend(frameon=False)\n",
    "plt.axvline(start_constant_alt-tr.stats.starttime, linestyle=':', color='black')\n",
    "plt.axvline(end_constant_alt-tr.stats.starttime, linestyle=':', color='black')\n",
    "plt.xlabel(f'Time since {tr.stats.starttime}')\n",
    "plt.ylabel('Altitude (m)')\n",
    "\n",
    "tr_upper.trim(starttime=start_constant_alt, endtime=end_constant_alt)\n",
    "\n",
    "\n",
    "tr_upper.write(\"../2023_Celine_internship/msc_celine_specfem/utils/utils_NORSAR/test_data_Venus/2019_noise_balloon_GPS.mseed\", format=\"MSEED\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
