{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "broadband-measure",
   "metadata": {},
   "source": [
    "# ML-ready data generation\n",
    "We read pressure data from historical balloon flights, pre-process them, and store them as waveform snippets of the same length in .h5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "electronic-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from obspy.core.utcdatetime import UTCDateTime\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import obspy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "alert-smith",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hare2\n",
      "CrazyCatLower\n",
      "Hare\n",
      "CrazyCatUpper\n",
      "Hare\n",
      "Hare2\n",
      "Tortoise\n",
      "Tortoise\n",
      "CrazyCatUpper\n",
      "CrazyCatLower\n"
     ]
    }
   ],
   "source": [
    "def create_one_obspy_trace(times, amp, balloon, starttime, dt, target_sampling=1.):\n",
    "                        \n",
    "    #f = interpolate.interp1d(times, amp, kind='linear', )\n",
    "    #times_interp = np.arange(one_tec_data.epoch.values.min()*dt, one_tec_data.epoch.values.max()*dt+dt, dt)\n",
    "    #vTEC = f(times_interp)\n",
    "    tr = obspy.Trace()\n",
    "    tr.data = amp\n",
    "    tr.stats.delta = dt\n",
    "    tr.interpolate(sampling_rate=target_sampling)\n",
    "    #tr.stats.network = station\n",
    "    #tr.stats.station = satellite+'ZZZ'+station\n",
    "    tr.stats.station = balloon\n",
    "    tr.stats.starttime = starttime+times[0]\n",
    "    return tr\n",
    "\n",
    "def load_balloon_data(dir_data, starttimes, target_sampling=1.):\n",
    "    \n",
    "    datas = {'GPS': obspy.Stream(), 'Baro': obspy.Stream()}\n",
    "    for subdir, dirs, files in os.walk(dir_data):\n",
    "        #print(files)\n",
    "        for file in files:\n",
    "            filepath = subdir + os.sep + file\n",
    "            if not '.csv' in file:\n",
    "                continue\n",
    "                \n",
    "            #if not 'Tortoise' in file:\n",
    "            #    continue\n",
    "            \n",
    "            balloon = file.split('_')[0]\n",
    "            if not balloon in starttimes.keys():\n",
    "                continue\n",
    "            print(balloon)\n",
    "            \n",
    "            data = pd.read_csv(filepath, header=[0])\n",
    "            \n",
    "            type_data = file.split('_')[1].split('.')[0]\n",
    "            \n",
    "            starttime = starttimes[balloon]\n",
    "            times = data['GPSTime(s)'].values\n",
    "            #if balloon == 'Tortoise':\n",
    "            #    print(file)\n",
    "            #    print(times/3600)\n",
    "            dt = times[1]-times[0]\n",
    "            #print(data.columns)\n",
    "            try:\n",
    "                amp = data['WGS84Altitude(m)'].values\n",
    "            except:\n",
    "                amp = data[data.columns[-1]].values\n",
    "            tr_data = create_one_obspy_trace(times, amp, balloon, starttime, dt, target_sampling=target_sampling)\n",
    "            datas[type_data] += tr_data\n",
    "            \n",
    "    return datas\n",
    "\n",
    "starttimes = {\n",
    "    'Hare': UTCDateTime(2019, 7, 22),\n",
    "    'Tortoise': UTCDateTime(2019, 7, 22),\n",
    "    'Hare2': UTCDateTime(2019, 8, 9),\n",
    "    'CrazyCatLower': UTCDateTime(2019, 8, 9),\n",
    "    'CrazyCatUpper': UTCDateTime(2019, 8, 9),\n",
    "}\n",
    "target_sampling = 3.\n",
    "dir_data = '/staff/quentin/Documents/Projects/2020_Ridgecrest/data_balloons/Siddharth_balloon/'\n",
    "st_all = load_balloon_data(dir_data, starttimes, target_sampling=target_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "supposed-camping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrazyCatLower\n",
      "Length to process: 1344\n",
      "Hare\n",
      "Length to process: 388\n",
      "CrazyCatUpper\n",
      "Length to process: 2897\n",
      "Hare2\n",
      "Length to process: 1179\n",
      "Tortoise\n",
      "Length to process: 403\n"
     ]
    }
   ],
   "source": [
    "def trim_st(st_all, duration, overlap=1., get_GPS=True):\n",
    "    \n",
    "    new_st = {'GPS': obspy.Stream(), 'Baro': obspy.Stream()}\n",
    "    for tr_Baro in st_all['Baro']:\n",
    "        \n",
    "        balloon = tr_Baro.stats.station\n",
    "        print(balloon)\n",
    "        if get_GPS:\n",
    "            tr_GPS = st_all['GPS'].select(station=balloon)[0].copy()\n",
    "            tr_GPS.trim(starttime=tr_Baro.stats.starttime, endtime=tr_Baro.stats.endtime)\n",
    "            starttimes = np.arange(0., tr_GPS.times()[-1], duration*overlap) \n",
    "        else:\n",
    "            starttimes = np.arange(0., tr_Baro.times()[-1], duration*overlap)\n",
    "            \n",
    "        print('Length to process:', starttimes.size)\n",
    "        for id, starttime in enumerate(starttimes):\n",
    "            #print(id)\n",
    "            tr_loc_Baro = tr_Baro.copy().trim(starttime=tr_Baro.stats.starttime+starttime, endtime=tr_Baro.stats.starttime+starttime+duration)\n",
    "            tr_loc_Baro.stats.station = tr_loc_Baro.stats.station+'-'+str(id)\n",
    "            if get_GPS:\n",
    "                tr_loc_GPS = tr_GPS.copy().trim(starttime=tr_Baro.stats.starttime+starttime, endtime=tr_Baro.stats.starttime+starttime+duration)\n",
    "                tr_loc_GPS.stats.station = tr_loc_GPS.stats.station+'-'+str(id)\n",
    "            new_st['Baro'] += tr_loc_Baro\n",
    "            if get_GPS:\n",
    "                new_st['GPS'] += tr_loc_GPS\n",
    "    return new_st\n",
    "    \n",
    "from math import log, ceil, floor\n",
    "def closest_power(x, power=8):\n",
    "    possible_results = floor(log(x, power)), ceil(log(x, power))\n",
    "    return min(possible_results, key= lambda z: abs(x-power**z))\n",
    "   \n",
    "def find_closest_duration(target_duration, target_sampling):\n",
    "    nsize = int(target_sampling*target_duration)\n",
    "    power = closest_power(nsize, power=2)\n",
    "    nsize = 2**power\n",
    "    duration = nsize/target_sampling\n",
    "    return duration\n",
    "    \n",
    "target_duration = 50.\n",
    "overlap = 1\n",
    "duration = find_closest_duration(target_duration, target_sampling)\n",
    "new_st = trim_st(st_all, duration, overlap=overlap, get_GPS=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "every-vocabulary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset \"training\" (3726.0 inputs): 0 - 3726.0\n",
      "dataset \"validation\" (1863.0 inputs): 3726.0 - 5589.0\n",
      "dataset \"testing\" (621.0 inputs): 5589.0 - 6210.0\n",
      "--- training\n",
      "CrazyCatLower-1343\n",
      "(41, 1)\n",
      "Hare-387\n",
      "(25, 1)\n",
      "--- validation\n",
      "CrazyCatUpper-2896\n",
      "(122, 1)\n",
      "--- testing\n",
      "Hare2-1178\n",
      "(62, 1)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "def store_all_as_hdf5(datasets, st_all, target_size, crop_percent=0.6, freq_min=0.1):\n",
    "\n",
    "    ## Initialization dataset\n",
    "    size_crop = st_all['Baro'][0].data.size-int(st_all['Baro'][0].data.size*crop_percent)\n",
    "    \n",
    "    there_are_GPS_data = True if len(st_all['GPS']) > 0 else False\n",
    "    \n",
    "    # Create a subgroup for each stream with the corresponding event ID as the subgroup name\n",
    "    event_type = 'balloon'\n",
    "    for dataset in datasets:\n",
    "    \n",
    "        results = {'X': [], 'label': [], 'event_type': [], 'window': [], 'id': [], 'tr_id': [], 'station': [], 'satellite': []}\n",
    "        print('---', dataset)\n",
    "        \n",
    "        idmin, idmax = datasets[dataset]\n",
    "        idmin, idmax = int(idmin), int(idmax)\n",
    "        st_Baro_loc = st_all['Baro'][idmin:idmax]\n",
    "        if there_are_GPS_data:\n",
    "            st_GPS_loc = st_all['GPS'][idmin:idmax]\n",
    "        else:\n",
    "            st_GPS_loc = st_Baro_loc\n",
    "        #id = 0\n",
    "        #st_Baro_loc = st_all['Baro'][id:].copy()\n",
    "        #st_GPS_loc = st_all['GPS'][id:].copy()\n",
    "        for tr_Baro_loc_in, tr_GPS_loc in zip(st_Baro_loc, st_GPS_loc):\n",
    "            \n",
    "            \n",
    "            tr_Baro_loc = tr_Baro_loc_in.copy()\n",
    "            window = (str(tr_Baro_loc.stats.starttime), str(tr_Baro_loc.stats.endtime))\n",
    "            tr_Baro_loc.detrend()\n",
    "            tr_Baro_loc.filter('highpass', freq=freq_min)\n",
    "            tr_Baro_loc_cropped = tr_Baro_loc.copy()\n",
    "            if size_crop > 0:\n",
    "                tr_Baro_loc_cropped.data[size_crop//2:-size_crop//2] = 0.\n",
    "           \n",
    "            if False:\n",
    "                fig = plt.figure(figsize=(10,5))\n",
    "                grid = fig.add_gridspec(3, 1)\n",
    "\n",
    "                ax = fig.add_subplot(grid[:2, 0])\n",
    "                ax.plot(tr_Baro_loc.times(), tr_Baro_loc.data)\n",
    "                ax.plot(tr_Baro_loc_cropped.times(), tr_Baro_loc_cropped.data)\n",
    "                \n",
    "                ax = fig.add_subplot(grid[2, 0])\n",
    "                ax.plot(tr_GPS_loc.times(), tr_GPS_loc.data)\n",
    "                return\n",
    "            \n",
    "            \"\"\"\n",
    "            obs_time = UTCDateTime('2019-07-22T16:27:26')\n",
    "            if (tr_Baro_loc.stats.starttime<=obs_time) & (tr_Baro_loc.stats.endtime>=obs_time):\n",
    "                print('cool')\n",
    "                print(tr_Baro_loc.stats.station)\n",
    "                print(tr_Baro_loc.data)\n",
    "            \"\"\"\n",
    "            #X0 = np.expand_dims(tr_Baro_loc_cropped.data/abs(tr_Baro_loc_cropped.data).max(), axis=-1)\n",
    "            #X1 = np.expand_dims(tr_GPS_loc.data/abs(tr_GPS_loc.data).max(), axis=-1)\n",
    "            if abs(tr_Baro_loc.data).max() == 0.:\n",
    "                #print(tr_Baro_loc.stats.station)\n",
    "                #print(X0.shape, X1.shape)\n",
    "                continue\n",
    "            \n",
    "            X0 = np.expand_dims(tr_Baro_loc_cropped.data, axis=-1)\n",
    "            if there_are_GPS_data:\n",
    "                X1 = np.expand_dims(tr_GPS_loc.data, axis=-1)\n",
    "                if not X0.shape[0] == X1.shape[0]:\n",
    "                    #print(tr_Baro_loc.stats.station)\n",
    "                    #print(X0.shape, X1.shape)\n",
    "                    continue\n",
    "                X = np.concatenate((X0, X1), axis=-1)\n",
    "            else:\n",
    "                X = X0\n",
    "            X = X[:target_size,:]\n",
    "            label = np.expand_dims(tr_Baro_loc.data, axis=-1)\n",
    "            label = label[:target_size,:]\n",
    "            \n",
    "            if X.shape[0] < target_size or label.shape[0] < target_size:\n",
    "                print(tr_Baro_loc.stats.station)\n",
    "                print(X.shape)\n",
    "                continue\n",
    "            \n",
    "            if np.isnan(X).any():\n",
    "                #print('problem nan')\n",
    "                #print(tr_Baro_loc.stats.station)\n",
    "                #print(X.shape)\n",
    "                continue\n",
    "                \"\"\"\n",
    "                fig = plt.figure(figsize=(10,5))\n",
    "                grid = fig.add_gridspec(3, 1)\n",
    "\n",
    "                ax = fig.add_subplot(grid[:2, 0])\n",
    "                ax.plot(tr_Baro_loc.times(), tr_Baro_loc.data)\n",
    "                ax.plot(tr_Baro_loc_cropped.times(), tr_Baro_loc_cropped.data)\n",
    "                \n",
    "                ax = fig.add_subplot(grid[2, 0])\n",
    "                ax.plot(tr_GPS_loc.times(), tr_GPS_loc.data)\n",
    "                print(tr_Baro_loc.data)\n",
    "                #print(X1)\n",
    "                return\n",
    "                \"\"\"\n",
    "            \n",
    "            results['X'].append( X )\n",
    "            results['label'].append( label )\n",
    "            results['event_type'].append( event_type )\n",
    "            results['window'].append( window )\n",
    "            results['id'].append( tr_Baro_loc.stats.station )\n",
    "        \n",
    "        # Open the HDF5 file in \"write\" mode\n",
    "        with h5py.File(filename.format(dataset=dataset), \"w\") as f:\n",
    "            f.create_dataset('X', data=results['X'], dtype='float32')\n",
    "            f.create_dataset('label', data=results['label'], dtype='float32')\n",
    "            f.create_dataset('event_type', data=str(results['event_type']))\n",
    "            f.create_dataset('window', data=results['window'])\n",
    "            f.create_dataset('id', data=results['id'])\n",
    "\n",
    "def prepare_datasets_dates(list_quantiles, metadata):\n",
    "    l_dates = metadata['starttime'].astype('int64').quantile(list_quantiles).astype('datetime64[ns]').values\n",
    "    datasets = {'training': (metadata['starttime'].min(), l_dates[0]), 'validation': (l_dates[0], l_dates[1]), 'testing': (l_dates[1], l_dates[2])}\n",
    "    for dataset in datasets:\n",
    "        tmin, tmax = datasets[dataset]\n",
    "        nb_elem = metadata.loc[(metadata.starttime>=tmin)&(metadata.starttime<=tmax)].shape[0]\n",
    "        print('dataset \"{}\" ({} inputs): {} - {}'.format(dataset, nb_elem, tmin, tmax))\n",
    "    return datasets\n",
    "\n",
    "def prepare_datasets_ids(list_quantiles, new_st):\n",
    "    l_ids = np.arange(len(new_st['Baro']))\n",
    "    l_bound_ids = np.quantile(l_ids, q=list_quantiles)\n",
    "    datasets = {'training': (0, l_bound_ids[0]), 'validation': (l_bound_ids[0], l_bound_ids[1]), 'testing': (l_bound_ids[1], l_bound_ids[2])}\n",
    "    for dataset in datasets:\n",
    "        idmin, idmax = datasets[dataset]\n",
    "        nb_elem = idmax-idmin\n",
    "        print('dataset \"{}\" ({} inputs): {} - {}'.format(dataset, nb_elem, idmin, idmax))\n",
    "    return datasets\n",
    "        \n",
    "filename = '/projects/active/infrasound/data/infrasound/2023_ML_balloon/data/{dataset}_waveform_dataset.h5'\n",
    "list_quantiles = [0.6, 0.9, 1.]\n",
    "datasets = prepare_datasets_ids(list_quantiles, new_st)\n",
    "target_size=int(duration*target_sampling)\n",
    "results = store_all_as_hdf5(datasets, new_st, target_size, crop_percent=0., freq_min=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-action",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
